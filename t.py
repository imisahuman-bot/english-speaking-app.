import streamlit as st
import requests
import base64
from gtts import gTTS
import os
from io import BytesIO
from openai import OpenAI

# ==============================
# SETUP
# ==============================
st.set_page_config(page_title="AI Pronunciation Coach", page_icon="üé§")
st.title("üé§ AI Pronunciation Coach")
st.write("H√£y n√≥i m·ªôt c√¢u ti·∫øng Anh, AI s·∫Ω nghe, ch·∫•m ƒëi·ªÉm v√† ph·∫£n h·ªìi b·∫±ng gi·ªçng n√≥i!")

# T·∫°o client OpenAI (ƒë·∫£m b·∫£o anh ƒë√£ set API key trong Streamlit Cloud secret)
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# ==============================
# HTML + JS Recorder
# ==============================
st.markdown("""
    <style>
    .recorder-container {
        text-align: center;
        margin-top: 20px;
    }
    button {
        background-color: #4CAF50;
        border: none;
        color: white;
        padding: 10px 24px;
        font-size: 16px;
        border-radius: 8px;
    }
    </style>
""", unsafe_allow_html=True)

record_html = """
<div class="recorder-container">
  <button id="recordButton">üéôÔ∏è Start Recording</button>
  <p id="status"></p>
  <audio id="audioPlayback" controls></audio>
</div>

<script>
  let chunks = [];
  let recorder;
  const recordButton = document.getElementById('recordButton');
  const status = document.getElementById('status');
  const audioPlayback = document.getElementById('audioPlayback');

  recordButton.onclick = async () => {
    if (recordButton.textContent.includes('Start')) {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      recorder = new MediaRecorder(stream);
      chunks = [];

      recorder.ondataavailable = e => chunks.push(e.data);
      recorder.onstop = e => {
        const blob = new Blob(chunks, { type: 'audio/webm' });
        const audioURL = URL.createObjectURL(blob);
        audioPlayback.src = audioURL;

        // G·ª≠i blob v·ªÅ Streamlit
        const reader = new FileReader();
        reader.readAsDataURL(blob);
        reader.onloadend = () => {
          const base64data = reader.result.split(',')[1];
          window.parent.postMessage({ type: 'streamlit:setComponentValue', value: base64data }, '*');
        };
      };

      recorder.start();
      recordButton.textContent = '‚èπ Stop Recording';
      status.textContent = 'Recording...';
    } else {
      recorder.stop();
      recordButton.textContent = 'üéôÔ∏è Start Recording';
      status.textContent = 'Stopped.';
    }
  };
</script>
"""

base64_audio = st.components.v1.html(record_html, height=300)

# ==============================
# HANDLE AUDIO & AI RESPONSE
# ==============================
if base64_audio:
    st.success("‚úÖ ƒê√£ thu √¢m xong! ƒêang x·ª≠ l√Ω...")

    # L∆∞u file t·∫°m
    audio_bytes = base64.b64decode(base64_audio)
    with open("temp_audio.webm", "wb") as f:
        f.write(audio_bytes)

    # --------------------
    # 1Ô∏è‚É£ Whisper: Chuy·ªÉn gi·ªçng n√≥i th√†nh vƒÉn b·∫£n
    # --------------------
    with open("temp_audio.webm", "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="gpt-4o-mini-transcribe",
            file=audio_file
        )
    user_text = transcript.text
    st.write(f"üó£Ô∏è **B·∫°n n√≥i:** {user_text}")

    # --------------------
    # 2Ô∏è‚É£ GPT: Ch·∫•m ƒëi·ªÉm & ph·∫£n h·ªìi
    # --------------------
    prompt = f"""
    B·∫°n l√† gi√°o vi√™n ti·∫øng Anh. H√£y nghe ƒëo·∫°n ph√°t √¢m sau:
    "{user_text}"
    ƒê√°nh gi√° m·ª©c ƒë·ªô ph√°t √¢m (t·ª´ 1 ƒë·∫øn 10), nh·∫≠n x√©t l·ªói ch√≠nh, v√† g·ª£i √Ω c√°ch ph√°t √¢m t·ªët h∆°n.
    Vi·∫øt b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn.
    """

    feedback = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "B·∫°n l√† gi√°o vi√™n d·∫°y ph√°t √¢m ti·∫øng Anh."},
            {"role": "user", "content": prompt}
        ]
    )

    feedback_text = feedback.choices[0].message.content
    st.markdown(f"### üí¨ Ph·∫£n h·ªìi t·ª´ AI:\n{feedback_text}")

    # --------------------
    # 3Ô∏è‚É£ gTTS: ƒê·ªçc l·∫°i ph·∫£n h·ªìi
    # --------------------
    tts = gTTS(text=feedback_text, lang="vi")
    tts_buffer = BytesIO()
    tts.write_to_fp(tts_buffer)
    tts_buffer.seek(0)

    audio_base64 = base64.b64encode(tts_buffer.read()).decode()
    audio_html = f"""
    <audio controls autoplay>
        <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
    </audio>
    """
    st.markdown(audio_html, unsafe_allow_html=True)

    # X√≥a file t·∫°m
    os.remove("temp_audio.webm")
